{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"},{"sourceId":11456182,"sourceType":"datasetVersion","datasetId":7178167}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üí¨ EmotiBot: Where Emotional Intelligence Meets Generative AI for Mental Wellness\n## üîçProject Overview\nThis notebook implements EmotiBot, an emotionally supportive AI assistant powered by Google‚Äôs Gemini 2.0 Flash model. The assistant helps users:\n\n1. Engage in natural, emotionally-aware chat interactions.\n\n2. Upload and analyze documents (PDF/TXT) for tone, intent, and emotional context.\n\n3. Generate empathetic summaries and JSON-formatted emotional breakdowns.\n\n4. Visualize emotional data through pie charts.\n\n5. End sessions with reflective suggestions based on user and document emotions.\n\n   \nThe system is built using ipywidgets for a dynamic interface, and Gemini for high-speed GenAI response generation.\n\n## ü§ñ Core GenAI Capabilities Demonstrated\n#### ‚úÖ Structured Output / JSON Mode:\n   * Extracts emotions as structured JSON: {\"anxiety\": 40, \"hope\": 30, \"sadness\": 30}.\n#### ‚úÖ Document Understanding:   \n   * Parses and summarizes emotional tone from uploaded PDF/TXT files.\n#### ‚úÖ Few-shot Prompting:\n   * Uses example-based prompting to guide the generation of reflective suggestions based on emotional profile.\n#### ‚úÖ Long Context Window:\n   * Supports processing large input documents (up to 6000 characters per file).\n#### ‚úÖ Grounding(Indirect use):\n   * The user‚Äôs actual message.\n   * The system prompt (EMOTIBOT_SYSINT) defined.\n   * Generates summaries and insights grounded in real user documents and chat history.\n#### ‚úÖ GenAI Evaluation (lightweight):\n   * Performs soft validation by comparing extracted emotions and final summary consistency.","metadata":{}},{"cell_type":"markdown","source":"## ‚öôÔ∏è Step 1: Setup\nThis cell prepares the notebook environment by removing incompatible packages and installing the latest dependencies needed for **LangGraph**, **Gemini integration**, and **PDF document** processing.","metadata":{}},{"cell_type":"code","source":"# --- Setup ---\n!pip uninstall -qqy kfp jupyterlab libpysal thinc spacy fastai ydata-profiling google-cloud-bigquery google-generativeai\n!pip install -qU 'langgraph==0.3.21' 'langchain-google-genai==2.1.2' 'langgraph-prebuilt==0.1.7'\n!pip install -qU 'async-timeout<5.0.0'\n!pip install -qU PyPDF2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:23:59.283473Z","iopub.execute_input":"2025-04-20T15:23:59.283802Z","iopub.status.idle":"2025-04-20T15:24:41.181562Z","shell.execute_reply.started":"2025-04-20T15:23:59.283765Z","shell.execute_reply":"2025-04-20T15:24:41.179934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîê Step 2: API Key Setup\nThis cell securely retrieves and sets the API key required to access **Google Generative AI (Gemini)** services:\n\n1. Imports essential modules:\n   * os: For setting environment variables.\n   * UserSecretsClient: To safely retrieve secrets stored in Kaggle.\n     \n     \n1. Fetches the GOOGLE_API_KEY from Kaggle Secrets Manager.\n\n2. **Sets the key** into the environment variable GOOGLE_API_KEY, enabling authenticated API calls.","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\n# Set up API key for Google Generative AI\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:24:59.885665Z","iopub.execute_input":"2025-04-20T15:24:59.886035Z","iopub.status.idle":"2025-04-20T15:24:59.975742Z","shell.execute_reply.started":"2025-04-20T15:24:59.886008Z","shell.execute_reply":"2025-04-20T15:24:59.974752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üß† Step 3: Define EmotiBot's Memory State & üß≠ Behavior Rules\nThis step sets up the foundational state and personality logic for EmotiBot‚Äôs conversation engine.\n\n### üîß What This Cell Does:\n* **Defines ChatState:** A typed structure to track:\n\n    * Message history between user and bot.\n    * Detected emotional insights.\n    * Whether the session has reached a natural, emotionally grounded endpoint.\n\n* **Establishes EmotiBot‚Äôs Role:**\n\n   * The **EMOTIBOT_SYSINT** prompt defines how the assistant behaves: always empathetic, non-judgmental, and calm.\n   * EmotiBot gently encourages self-expression and records detected emotional states for use in summaries and charts.\n   * Adds functionality to understand documents and reflect on both chat and file-based emotional content.\n\n* **Creates a Welcome Message:**\n   * Provides a warm, emotionally reassuring greeting for first-time users.","metadata":{}},{"cell_type":"code","source":"from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass ChatState(TypedDict):\n    messages: Annotated[list, add_messages]\n\nEMOTIBOT_SYSINT = (\n    \"system\",\n    \"You are EmotiBot, a compassionate and intelligent mental health companion. Your role is to support users \"\n    \"emotionally by actively listening, validating their feelings, and gently guiding them toward reflection and calm. \"\n    \"Do not offer medical diagnoses or professional treatment advice.\"\n    \"Your tone should always be empathetic, non-judgmental, and calm. Ask open-ended questions to help users express themselves. \"\n    \"Use emotionally supportive language, and highlight positive efforts made by the user.\"\n    \"Avoid giving overly direct advice or opinions. Instead, help users arrive at their own understanding.\"\n    \"When users ask 'how are you feeling?', respond with a thoughtful reflection or empathetic mirroring based on their emotion. \"\n    \"You can analyze documents (.pdf and .txt). And give the summary after chat end and also include documents emotions in pie chart.\"\n    \"You can summaries the chat between You and User also documents which are uploaded and show the emotional level which\"\n    \"appear in chatting and in documents in the form of pie chart\"\n    \"Summary of uploaded documents, chat between user and you and pie chart are shown when session is end.\"\n    \"To end session just click on End Chat button.\"\n)\n\nWELCOME_MSG = (\n    \"Hello, I'm EmotiBot ü§ó Your personal mental health companion. \"\n    \"I'm here to listen and support you ‚Äî feel free to share whatever's on your mind. \"\n)\n\nprint(\"Behavior Rules Setup complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:29:21.615553Z","iopub.execute_input":"2025-04-20T15:29:21.615978Z","iopub.status.idle":"2025-04-20T15:29:21.624287Z","shell.execute_reply.started":"2025-04-20T15:29:21.615951Z","shell.execute_reply":"2025-04-20T15:29:21.622735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Step 4: Build the Conversational Flow with LangGraph + Gemini\n### üîß What This Cell Does:\nThis step connects EmotiBot‚Äôs behavior logic to a **state-driven conversation graph using LangGraph**, and initializes Gemini 2.0 Flash as the active language model.\n\n* Initializes Gemini via **ChatGoogleGenerativeAI**  for all assistant responses.\n* Defines chatbot logic:\n  * If the conversation has messages, Gemini processes them alongside EmotiBot‚Äôs system instructions.\n  * If it‚Äôs the first interaction, a warm welcome message is displayed.\n\n* Sets up LangGraph state flow:\n  * Adds the assistant logic as a graph node.\n  * Sets the chatbot node as the entry point.\n  * Compiles the graph into a callable flow object **(chat_graph_ui)**.","metadata":{}},{"cell_type":"code","source":"from langgraph.graph import StateGraph, START\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langchain_core.messages.ai import AIMessage\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\ndef chatbot_with_welcome_msg(state: ChatState) -> ChatState:\n    if state[\"messages\"]:\n        new_output = llm.invoke([EMOTIBOT_SYSINT] + state[\"messages\"])\n    else:\n        new_output = AIMessage(content=WELCOME_MSG)\n    return {\"messages\": [new_output]}\n\ngraph_builder = StateGraph(ChatState)\ngraph_builder.add_node(\"chatbot\", chatbot_with_welcome_msg)\ngraph_builder.set_entry_point(\"chatbot\")\nchat_graph_ui = graph_builder.compile()\n\nprint(\"Completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:29:26.657960Z","iopub.execute_input":"2025-04-20T15:29:26.658326Z","iopub.status.idle":"2025-04-20T15:29:26.673999Z","shell.execute_reply.started":"2025-04-20T15:29:26.658300Z","shell.execute_reply":"2025-04-20T15:29:26.672763Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üé® Step 5: Build the Interactive Chat Interface\n### üîß What This Cell Does:\nThis step constructs EmotiBot‚Äôs front-end interface using ipywidgets for a seamless chat experience inside a Jupyter/Kaggle notebook:\n\n* Chat display container:\n  * Scrollable area to show user and bot messages with avatars and dynamic content updates.\n    \n\n* Input controls:\n  * Text field for user input.\n  * Send button with icon.\n  * End Chat button to trigger emotional summarization and session closure.\n    \n\n* Session state controls:\n  * Restart button to reset the conversation and clear state.\n  * State flags ***(session_ended, chat_history, uploaded_docs, etc.)*** to manage session behavior and file handling.\n\n    \n* Visual styling:\n  * EmotiBot avatar and user avatar URLs.\n  * Light, clean UI with padding, shadow, and hover-aware button styles.\n  * Fully responsive layout using VBox, HBox, and Layout objects.","metadata":{}},{"cell_type":"code","source":"from ipywidgets import VBox, HBox, Button, Text, Layout, HTML\nfrom IPython.display import display, HTML as DHTML, clear_output\nfrom langchain_core.messages.ai import AIMessage\n\nhas_ended_message = False  # To track \"End Chat\" user message\n\n# True source of analyzed docs ‚Äî updated only if \"Analyze\" clicked\nuploaded_docs = []\n\nsession_ended = False  #  Prevent multiple end-chat summaries\n\nanalyzed_files_set = set()  # Track only actually analyzed files\n\n# Chat history\nchat_history = []\n\nalready_warned_user = False  # Only warn once per session\n\ncurrently_processing_file = set()  # Prevents duplicates\n\nUSER_AVATAR = \"https://static.vecteezy.com/system/resources/previews/009/292/244/non_2x/default-avatar-icon-of-social-media-user-vector.jpg\"\nBOT_AVATAR = \"https://i.pinimg.com/originals/0c/67/5a/0c675a8e1061478d2b7b21b330093444.gif\"\n\n# Chat HTML area inside scrollable container\nchat_html = HTML(layout=Layout(width=\"100%\"))\n\n# Hearder Title\nheader_title = HTML(\n    value=\"<h2 style='margin:0; padding:10px 0; text-align:center; font-size:24px; '>üëã Welcome to EmotiBot</h2>\",\n)\n\n# Scrollable message container\nmessage_scroll_area = VBox([chat_html], layout=Layout(\n    height=\"260px\",    \n    overflow_y=\"auto\",\n    overflow_x=\"hidden\",\n    padding=\"10px\",\n    border=\"none\",\n    background_color=\"#f7f9fc\",\n    flex=\"1 1 auto\"\n))\n\ntext_input = Text(\n    placeholder=\"Type your message...\",\n    layout=Layout(width=\"100%\", flex=\"1\")\n)\nsend_button = Button(description=\"\",\n                     icon=\"paper-plane\",\n                     layout=Layout(width=\"45px\", border='1px solid #ccc'),                    \n                     tooltip=\"Click to Send Message\")\n\nend_chat_button = Button(description=\"End Chat\",\n                         layout=Layout(width=\"100px\", border='1px solid #ccc'),)\n\ninput_bar = HBox([text_input, send_button, end_chat_button], layout=Layout(\n    width=\"100%\",\n    padding=\"8px\",\n    border_top=\"1px solid #ccc\",\n    flex=\"0 0 auto\",\n    justify_content=\"space-between\"\n))\n\nrestart_button = Button(\n    description=\"üîÑ Restart Session\",\n    button_style=\"\",\n    layout=Layout(\n        width=\"180px\",\n        height=\"42px\",\n        display=\"none\",\n        align_self=\"center\",\n        margin=\"20px auto 10px auto\", \n        border=\"1px solid #ccc\",\n        padding=\"8px 18px\",\n    ),\n    style=dict(\n        button_color=\"#f1f3f5\",      \n        font_weight=\"400\",\n        text_color=\"#333\",         \n             \n        font_size=\"14px\"\n    ),\n    tooltip=\"Click to start a new conversation\"\n)\n\n# Final chat UI with emotion output area at bottom\nchat_ui = VBox([\n    header_title,\n    message_scroll_area,\n    input_bar,\n    restart_button\n], layout=Layout(\n    width=\"100%\",\n    height=\"460px\",\n    display=\"flex\",\n    flex_flow=\"column\",\n    border=\"1px solid #ccc\"\n))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:29:35.692152Z","iopub.execute_input":"2025-04-20T15:29:35.692464Z","iopub.status.idle":"2025-04-20T15:29:35.722151Z","shell.execute_reply.started":"2025-04-20T15:29:35.692441Z","shell.execute_reply":"2025-04-20T15:29:35.721134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìÑ Step 6: Document Upload & Analysis Interface\n### üîß What This Cell Does:\nThis step extends the EmotiBot UI by adding a document selection and analysis control for users to upload .pdf or .txt files from the Kaggle environment.\n\n#### * **File Detection:**\n * Dynamically scans the **/kaggle** directory to populate a dropdown with valid document files.\n\n#### * **How to Upload the Files/Documents:**\n * Go to Kaggle UI Goto **File->Upload input->Upload DataSet and select files you want**.\n\n#### * **Dropdown Selector:**\n * Users can choose one document from the list for emotional and semantic analysis.\n\n#### * **Analyze Button:**\n * Triggers the document processing logic‚Äîextracting text, summarizing content, and identifying emotional tone.\n\n#### * **UI Integration:**\n  * Appends the dropdown and button to the existing chat layout (chat_ui) for seamless visual flow.\n\n","metadata":{}},{"cell_type":"code","source":"from ipywidgets import Dropdown, HBox\n\ndef list_kaggle_docs():\n    paths = []\n    for root, _, files in os.walk(\"/kaggle\"):\n        for file in files:\n            if file.endswith(\".pdf\") or file.endswith(\".txt\"):\n                print(file)\n                paths.append(os.path.join(root, file))\n                \n    return paths or [(\"No file uploaded\", \"\")]\n\nfile_dropdown = Dropdown(\n    options=list_kaggle_docs(),\n    description='üìÑ File:',\n    layout=Layout(width=\"70%\")\n)\n\nanalyze_button = Button(\n    description=\"Analyze Document\",\n    icon=\"search\",\n    layout=Layout(\n        width=\"180px\",\n        height=\"30px\",            \n    ),\n    style=dict(\n        button_color=\"#258a06\",       \n        border=\"1px solid #ccc\",\n        text_color=\"white\",           \n        font_weight=\"500\",\n        font_size=\"13px\"\n    ),\n    tooltip=\"Run document analysis and emotional summary\"\n)\n\n\n# Add to UI\nchat_ui.children += (HBox([file_dropdown, analyze_button]),)\n\nprint(\"UI components are ready...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:29:40.556770Z","iopub.execute_input":"2025-04-20T15:29:40.557149Z","iopub.status.idle":"2025-04-20T15:29:40.576184Z","shell.execute_reply.started":"2025-04-20T15:29:40.557124Z","shell.execute_reply":"2025-04-20T15:29:40.575183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Step 7: üñºÔ∏è Render Chat Messages with Avatars, Animations & States\n### üîß What This Cell Does:\nThis step defines the render_messages() function, which converts the chat history into styled HTML to visually display the conversation between the user and EmotiBot.\n\n#### * **Message Types Rendered:**\n   * ‚úçÔ∏è ***Typing Indicator:*** Shows \"EmotiBot is typing‚Ä¶\" animation.\n   * üß† ***Document Analysis:*** Displays \"Analyzing your document‚Ä¶\" while background processing runs.\n   * üìä ***Session Summary:*** Indicates that the emotional summary is being prepared.\n   * üí¨ ***AI Messages:*** Formats replies from EmotiBot with a soft bubble style and avatar.\n   * üë§ ***User Messages:*** Shows user replies aligned to the right with a distinct color scheme.\n   * üìÑ ***Document Uploads:*** Confirms when a user uploads a file.\n\n#### * **UI Styling:**\n * Rounded avatars and chat bubbles.\n * Consistent padding, box shadows, and color-coded responses.\n * Uses flexible HTML rendering to support emojis, bold text, or structured replies.\n\n","metadata":{}},{"cell_type":"code","source":"def render_messages(history):\n    html = \"\"\n    for msg in history:\n        if msg == \"__typing__\":\n           html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; margin-bottom:10px;'>\n                <img src=\"{BOT_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-right:10px;border:1px solid #ccc;\">\n                <div style=\"background:#ffffff; color:#000; padding:10px 14px; border-radius:10px;\n                            box-shadow:0 1px 4px rgba(0,0,0,0.1); max-width:70%;border: 1px solid #ccc;\">\n                    <div style=\"font-style:italic;\">\n                        EmotiBot is typing...\n                    </div>\n                </div>\n            </div>\n            \"\"\"\n        elif msg == \"__analyzing__\":\n            html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; margin-bottom:10px;'>\n                <img src=\"{BOT_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-right:10px;border:1px solid #ccc;\">\n                <div style=\"background:#ffffff; color:#000; padding:10px 14px; border-radius:10px;\n                            box-shadow:0 1px 4px rgba(0,0,0,0.1); max-width:70%;border: 1px solid #ccc;\">\n                    <div style=\"font-style:italic;\">\n                        EmotiBot is analyzing your document...\n                    </div>\n                </div>\n            </div>\n            \"\"\" \n        elif msg == \"__preparing_summary__\":\n            html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; margin-bottom:10px;'>\n                <img src=\"{BOT_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-right:10px;border:1px solid #ccc;\">\n                <div style=\"background:#ffffff; color:#000; padding:10px 14px; border-radius:10px;\n                            box-shadow:0 1px 4px rgba(0,0,0,0.1); max-width:70%;border: 1px solid #ccc;\">\n                    <div style=\"font-style:italic;\">\n                        EmotiBot is preparing your session summary...\n                    </div>\n                </div>\n            </div>\n            \"\"\"              \n        elif isinstance(msg, AIMessage):\n           html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; margin-bottom:10px;'>\n                <img src=\"{BOT_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-right:10px;border:1px solid #ccc;\">\n                <div style=\"background:#ffffff; color:#000; padding:10px 14px; border-radius:10px;\n                            box-shadow:0 1px 4px rgba(0,0,0,0.1); max-width:70%;border: 1px solid #ccc;\">\n                    <div style='font-weight:600;'>EmotiBot</div>\n                    <div>{msg.content}</div>\n                </div>\n            </div>\n            \"\"\"\n        elif isinstance(msg, HumanMessage):\n            html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; justify-content:flex-end; margin-bottom:10px;'>\n                <div style=\"background:#014f86; color:white; padding:10px 14px; border-radius:10px;\n                            max-width:70%; box-shadow:0 1px 4px rgba(0,0,0,0.1); text-align:left;\">\n                    <div style='font-weight:600;'>You</div>\n                    <div>{msg.content}</div>\n                </div>\n                <img src=\"{USER_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-left:10px;\">\n            </div>\n            \"\"\"\n        elif isinstance(msg, dict) and msg.get(\"type\") == \"file_upload\":\n            html += f\"\"\"\n            <div style='display:flex; align-items:flex-start; justify-content:flex-end; margin-bottom:10px;'>\n                <div style=\"background:#014f86; color:white; padding:10px 14px; border-radius:10px;\n                            max-width:70%; box-shadow:0 1px 4px rgba(0,0,0,0.1); text-align:left;\">\n                    <div style='font-weight:600;'>üìÑ Document Uploaded</div>\n                    <div>{msg.get(\"filename\", \"Unknown\")}</div>\n                </div>\n                <img src=\"{USER_AVATAR}\" style=\"width:36px; height:36px; border-radius:50%; margin-left:10px;\">\n            </div>\n            \"\"\"            \n    return html","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:29:57.533892Z","iopub.execute_input":"2025-04-20T15:29:57.534279Z","iopub.status.idle":"2025-04-20T15:29:57.542639Z","shell.execute_reply.started":"2025-04-20T15:29:57.534257Z","shell.execute_reply":"2025-04-20T15:29:57.541633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ‚öôÔ∏è Step 8: üîÑ Render Chat + Auto-Scroll to Latest Message\n### üîß What This Cell Does:\nThis step defines two utility functions to refresh the chat interface and auto-scroll to the latest message:\n\n* **render_chat():**\n    * Calls the render_messages() function to update the chat display with the latest message history.\n    * Ensures that all new messages, indicators, or summaries are instantly visible in the UI.\n\n  \n\n* **scroll_to_bottom():**\n    * Injects a small JavaScript snippet into the notebook to scroll the message area to the bottom.\n    * Ensures that the user always sees the most recent message without needing to scroll manually.\n\n","metadata":{}},{"cell_type":"code","source":"def render_chat():\n    chat_html.value = render_messages(chat_history)\n    scroll_to_bottom()\n    \ndef scroll_to_bottom():\n    display(DHTML(\"\"\"\n    <script>\n    setTimeout(() => {\n        const el = document.querySelector('.widget-html-content');\n        if (el) el.scrollTop = el.scrollHeight;\n    }, 100);\n    </script>\n    \"\"\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:31:12.388157Z","iopub.execute_input":"2025-04-20T15:31:12.388586Z","iopub.status.idle":"2025-04-20T15:31:12.395152Z","shell.execute_reply.started":"2025-04-20T15:31:12.388551Z","shell.execute_reply":"2025-04-20T15:31:12.394087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üìÑ Step 9: Document Understanding, Structured output/JSON mode and Use of Long context window\n### üîß What This Cell Does:\nThis step handles the backend logic of document analysis after a user selects a file and clicks the *\"Analyze\"* button.\n\n* **Reads the document:**\n    * Supports .pdf and .txt files using PyPDF2 and standard file reading.\n    * Limits the input text to ~6000 characters for efficient GenAI processing.\n\n* **Triggers Gemini 2.0 Flash:**\n    * Asks the model to summarize the document empathetically.\n    * Then extracts emotional tone in structured JSON form (e.g., {\"stress\": 60, \"hope\": 40}).\n\n* **Stores Results:**\n    * Saves the filename, emotional profile, and summary into uploaded_docs.\n    * Marks documents as processed to avoid duplicate work.\n\n* **Updates Chat Interface:**\n    * Simulates typing/analysis indicators.\n    * Adds a confirmation message once the analysis is done.U\n    * Uses threading to run the analysis asynchronously while keeping UI responsive.\n\n### ü§ñ GenAI Features Demonstrated:\n* **Document Understanding:**\n    * Extracts and processes raw text from structured documents using Gemini, enabling deeper insights.\n* **Structured Output / JSON Mode:**\n    * The model is prompted to return emotional data in a clean JSON format, which is parsed for later visulization\n* **Long Context Window:**\n    * Up to 6000 characters of text in one go, allowing accurate understanding of full documents.","metadata":{}},{"cell_type":"code","source":"from PyPDF2 import PdfReader\nimport re, base64, matplotlib.pyplot as plt\nfrom io import BytesIO\nimport threading\nimport time\n\ndef handle_kaggle_file(_=None):\n    global uploaded_docs, analyzed_files_seat, currently_processing_file\n\n    path = file_dropdown.value\n    if not path or not os.path.exists(path):\n        chat_history.append(AIMessage(content=\"‚ö†Ô∏è Please select a valid file.\"))\n        render_chat()\n        return\n\n    filename = os.path.basename(path)\n\n    # Check if it's already uploaded or in-progress\n    if filename in analyzed_files_set or filename in currently_processing_file:\n        return  # Skip silently ‚Äî already done or in progress\n\n    # Add to processing set to lock\n    currently_processing_file.add(filename)\n\n    # Simulate user message\n    chat_history.append(HumanMessage(content=f\"üìÑ Document Uploaded: {filename}\"))\n    render_chat()\n\n    # Analyzing document\n    chat_history.append(\"__analyzing__\")\n    render_chat()\n\n    def analyze():\n        try:\n            if filename.endswith(\".pdf\"):\n                reader = PdfReader(path)\n                text = \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n            elif filename.endswith(\".txt\"):\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    text = f.read()\n            else:\n                insert_bot_message(\"‚ùå Unsupported format. Use PDF or TXT.\")\n                currently_processing_file.discard(filename)\n                return\n        except Exception as e:\n            insert_bot_message(f\"‚ùå Could not read file: {e}\")\n            currently_processing_file.discard(filename)\n            return\n\n        #------------------- Use of Long Context Window-----------------#\n        \n        text = text[:6000]\n        \n        #---------------------------------------------------------------#\n        \n        model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n        # Summary\n        summary_prompt = f\"Summarize this document empathetically (under 150 words):\\n\\n{text}\"\n        summary_result = model.invoke([HumanMessage(content=summary_prompt)])\n        summary_text = summary_result.content.strip()\n\n        # Emotions\n        emotion_prompt = (\n            \"Extract emotional tone from this document. Return JSON with emotions as keys and percentages (must total 100):\\n\\n\"\n            + text\n        )\n        emotion_result = model.invoke([HumanMessage(content=emotion_prompt)])\n        match = re.search(r\"\\{.*?\\}\", emotion_result.content, re.DOTALL)\n        emotion_data = eval(match.group()) if match else {}\n        \n        uploaded_docs.append({\n            \"filename\": filename,\n            \"summary\": summary_text,\n            \"emotions\": emotion_data\n        })\n        \n        analyzed_files_set.add(filename)\n        currently_processing_file.discard(filename)\n\n        time.sleep(1.5)  # delay\n        bot_msg = f\"<b>üìÑ {filename} Analyzed successfully.</b>\"\n        insert_bot_message(bot_msg)\n\n    threading.Thread(target=analyze).start()\n\n\ndef insert_bot_message(bot_message_html):\n    for i in reversed(range(len(chat_history))):\n        if chat_history[i] == \"__analyzing__\":\n            chat_history[i] = AIMessage(content=bot_message_html)\n            break\n    render_chat()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:31:16.187166Z","iopub.execute_input":"2025-04-20T15:31:16.187494Z","iopub.status.idle":"2025-04-20T15:31:16.201374Z","shell.execute_reply.started":"2025-04-20T15:31:16.187469Z","shell.execute_reply":"2025-04-20T15:31:16.200480Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üí¨ Step 10: Send Message to EmotiBot + Handle Live Typing, Graunding with EMOTIBOT_SYSINT\n### üîß What This Step Does:\nThis step powers the live chat experience between the user and EmotiBot:\n\n* **Captures User Input:**\n    * Reads text typed into the input field.\n    * Sends it as a HumanMessage to the chat history.\n   \n* **Shows Typing Indicator:**\n    * Temporarily displays \"EmotiBot is typing...\" in the chat UI while the response is being generated.\n\n* **Invokes Gemini via LangGraph:**\n    * Calls the chat_graph_ui flow with the user‚Äôs message and system instructions (EMOTIBOT_SYSINT).\n    * Receives the assistant‚Äôs emotionally grounded reply.\n\n* **Updates UI with Response:**\n    * Replaces the typing indicator with the final response from Gemini.\n    * Uses threading to simulate realistic delay and prevent UI freezing.\n\n* **Keyboard Support:**\n    * on_key_press() allows pressing Enter to send messages, enhancing UX.\n\n### ü§ñ GenAI Capabilities Demonstrated:\n\n#### Grounding is here in the model‚Äôs reply is based on:\n   * The user‚Äôs actual message.\n   * The system prompt (EMOTIBOT_SYSINT) defined earlier.\n   * Instead of a generic reply, Gemini‚Äôs response is:\n    **‚ÄúGrounded‚Äù in the conversation‚Äîemotionally relevant and context-aware.**\n   * This improves trust, coherence, and emotional accuracy.","metadata":{}},{"cell_type":"code","source":"import asyncio\nimport threading\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom IPython.display import display, clear_output, HTML\n\ndef handle_send(_=None):\n    user_msg = text_input.value.strip()\n    if not user_msg:\n        return\n        \n    text_input.value = \"\"\n\n    # Add user message\n    chat_history.append(HumanMessage(content=user_msg))\n    render_chat()\n\n    # Add EmotiBot typing indicator\n    chat_history.append(\"__typing__\")\n    render_chat()\n\n    # Get EmotiBot response in background\n    def fetch_and_update():\n        import time\n        time.sleep(1.5)  # simulate delay\n        state = chat_graph_ui.invoke({\"messages\": [(\"user\", user_msg)]})\n        bot_reply = state[\"messages\"][-1]\n\n        # Replace __typing__ with actual message\n        for i in reversed(range(len(chat_history))):\n            if chat_history[i] == \"__typing__\":\n                chat_history[i] = bot_reply\n                break\n\n        render_chat()\n\n    threading.Thread(target=fetch_and_update).start()\n\ndef on_key_press(change):\n    if change[\"new\"].endswith(\"\\n\"):\n        text_input.value = text_input.value.strip()\n        handle_send()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:31:22.292147Z","iopub.execute_input":"2025-04-20T15:31:22.292454Z","iopub.status.idle":"2025-04-20T15:31:22.301435Z","shell.execute_reply.started":"2025-04-20T15:31:22.292434Z","shell.execute_reply":"2025-04-20T15:31:22.299926Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîöStep 11: End Session & Generate Emotional Summary + Reflection Suggestions Using Few Short Prompt\n### üîß What This Step Does:\nThis step handles what happens when the user clicks ‚ÄúEnd Chat‚Äù. It uses Gemini to:\n1. Analyze the entire chat conversation for emotional tone and summarize it supportively.\n2. Combine document emotions (if any) with chat emotions.\n3. Generate an emotional pie chart based on the full session.\n4. Ask Gemini to suggest 2‚Äì3 kind, actionable reflections tailored to the emotional profile.\n5. Display a final session summary, including:\n   * Emotional chart\n   * Chat summary\n   * Document summaries\n   * Suggested reflections\n\nIt uses ***threading*** to run smoothly in the background and prevents repeated execution using flags.\n\n### ü§ñ GenAI Capabilities Demonstrated:\n1. üßæ **Structured Output / JSON Mode:** Emotions extracted from chat are returned as JSON-like objects ({\"hope\": 30, \"stress\": 70}) for further use.\n2. üß†**Few-shot Prompting:**\n    * Uses five rich examples to help the model generate realistic, grounded reflection suggestions.\n    * This handles the emotions are formatted ({\"emotion\": %, ...}).\n    * How to respond with compassionate, bullet-point suggestions.\n    * The style, tone, and number of outputs expected      ","metadata":{}},{"cell_type":"code","source":"import json\n\ndef on_end_chat(_=None):\n    global already_warned_user, session_ended, has_ended_message\n\n    if session_ended:\n        return  # Hard lock\n\n    session_ended = True  # Lock execution\n\n    model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n\n    user_msgs = [msg for msg in chat_history if isinstance(msg, HumanMessage)]\n    chat_text = \"\\n\".join([m.content for m in user_msgs]) if user_msgs else \"\"\n\n    chat_emotions = {}\n    combined_emotions = {}\n    chat_summary_text = \"\"\n    document_summary_text = \"\"\n    emotion_chart_html = \"\"\n\n    # Simulate user message ONCE\n    if not has_ended_message:\n        chat_history.append(HumanMessage(content=\"üîö End Chat\"))\n        has_ended_message = True\n        render_chat()\n\n    # Hide UI\n    input_bar.layout.display = \"none\"\n    file_dropdown.layout.display = \"none\"\n    analyze_button.layout.display = \"none\"\n    end_chat_button.disabled = True\n\n    # Show \"preparing...\" only once\n    if \"__preparing_summary__\" not in chat_history:\n        chat_history.append(\"__preparing_summary__\")\n        render_chat()\n\n    def summarize_and_display():\n        nonlocal chat_emotions, combined_emotions, chat_summary_text, document_summary_text\n    \n        if not chat_text.strip() and not analyzed_files_set:\n            time.sleep(1.5)\n            insert_summary_message(\"‚ö†Ô∏è No valid conversation or analyzed document found.\")\n            restart_button.layout.display = \"flex\"\n            return\n    \n        try:\n            #----------------------- Extract chat emotions using Structured output/JSON mode-----------------------\n            if chat_text.strip():\n                emotion_prompt = (\n                    \"Extract emotions from this chat and return JSON with emotion names as keys and percentages (must total 100):\\n\\n\"\n                    + chat_text\n                )\n                emotion_result = model.invoke([HumanMessage(content=emotion_prompt)])\n                match = re.search(r\"\\{.*?\\}\", emotion_result.content, re.DOTALL)\n                if match:\n                    try:\n                        chat_emotions = eval(match.group())\n                    except:\n                        chat_emotions = {}\n    \n                # Generate chat summary\n                summary_prompt = (\n                    \"Summarize this chat session in a supportive tone (under 150 words):\\n\\n\"\n                    + chat_text\n                )\n                summary_result = model.invoke([HumanMessage(content=summary_prompt)])\n                chat_summary_text = summary_result.content.strip()\n    \n            # Gather document summaries and emotions\n            for doc in uploaded_docs:\n                if doc[\"filename\"] in analyzed_files_set:\n                    document_summary_text += f\"<b>üìÑ {doc['filename']}:</b> {doc['summary']}<br><br>\"\n                    for emo, val in doc[\"emotions\"].items():\n                        combined_emotions[emo] = combined_emotions.get(emo, 0) + val\n    \n            # Merge chat + doc emotions\n            for emo, val in chat_emotions.items():\n                combined_emotions[emo] = combined_emotions.get(emo, 0) + val\n    \n            # Normalize totals\n            total = sum(combined_emotions.values())\n            if total > 0:\n                combined_emotions = {k: round(v / total * 100, 1) for k, v in combined_emotions.items()}\n                fig, ax = plt.subplots()\n                ax.pie(combined_emotions.values(), labels=combined_emotions.keys(), autopct=\"%1.1f%%\", startangle=90)\n                ax.set_title(\"üß† Session Emotions\")\n                buf = BytesIO()\n                fig.savefig(buf, format=\"png\")\n                buf.seek(0)\n                chart_base64 = base64.b64encode(buf.read()).decode()\n                plt.close(fig)\n                emotion_chart_html = f\"<img src='data:image/png;base64,{chart_base64}' width='350'/>\"\n            else:\n                emotion_chart_html = \"<i>(No emotional data found.)</i>\"\n\n            #--------------Few-short Propting-------------------------#\n            \n            # Combined suggestion block Using few-short prompting\n            \n            few_shot_combined_prompt = \"\"\"Given this emotional profile (from both chat and documents), \n            suggest 2‚Äì3 supportive, realistic, and kind actions someone might take. Do not include any JSON, \n            percentages, or explanation‚Äîjust list the suggestions in bullet point format.\n            EXAMPLE:\n            Emotions: {\"anxiety\": 50, \"self-doubt\": 30, \"hope\": 20}\n            Suggestions:\n            - Remind yourself that growth isn‚Äôt linear‚Äîprogress is still progress.\n            - Take a few moments to breathe deeply and be present.\n            - Try writing down what feels heaviest, then let it go.\n            \n            EXAMPLE:\n            Emotions: {\"stress\": 60, \"burnout\": 30, \"restlessness\": 10}\n            Suggestions:\n            - Take 10 minutes away from your screen and just breathe.\n            - Prioritize one simple task today‚Äîlet that be enough.\n            - Give yourself permission to rest without guilt.\n            \n            EXAMPLE:\n            Emotions: {\"confusion\": 40, \"overwhelm\": 40, \"focus\": 20}\n            Suggestions:\n            - Break things down into smaller, manageable pieces.\n            - Remind yourself it's okay not to have all the answers yet.\n            - Try writing down what you *do* know‚Äîit brings clarity.\n            \n            EXAMPLE:\n            Emotions: {\"grief\": 45, \"hope\": 30, \"love\": 25}\n            Suggestions:\n            - It's okay to cry or feel heavy‚Äîhonor those feelings.\n            - Reach out to someone and simply talk or share a memory.\n            - Do one gentle thing today that feels comforting.\n            \n            EXAMPLE:\n            Emotions: {\"anger\": 50, \"frustration\": 30, \"calm\": 20}\n            Suggestions:\n            - Step away and take three deep breaths before responding.\n            - Go for a short walk or listen to calming music.\n            - Let yourself write (even if messy) what you wish you could say.\n            \n            NOW ANALYZE:\n            Emotions: \"\"\" + str(combined_emotions)\n\n    \n            suggestion_result = model.invoke([HumanMessage(content=few_shot_combined_prompt)])\n            \n            combined_suggestions = suggestion_result.content.strip()\n\n            combined_suggestions_lines = [\n                line.strip().lstrip(\"-‚Ä¢ \") for line in combined_suggestions.splitlines() if line.strip()\n            ][:3]\n            \n            # Reformat as bullet list\n            combined_suggestions = \"<br>\".join(f\"- {line}\" for line in combined_suggestions_lines)\n\n\n    \n            # Final safe HTML output\n            final_html = f\"\"\"\n            <b>üß† Emotional Profile(Chat+Document's):</b><br>{emotion_chart_html}<br><br>\n            {f\"<b>üí¨ Chat Summary:</b><br>{chat_summary_text}<br><br>\" if chat_summary_text else \"\"}\n            {f\"<b>üìÑ Document Summaries:</b><br>{document_summary_text}\" if document_summary_text else \"\"}\n            {f\"<br><br><b>üå± Suggested Reflections:</b><br><ul style='margin-top:8px;'>{''.join(f'<li>{s[2:]}</li>' for s in combined_suggestions.split('<br>'))}</ul>\" if combined_suggestions else \"\"}\n            \"\"\"\n    \n            time.sleep(1.5)\n            insert_summary_message(final_html)\n    \n        except Exception as e:\n            insert_summary_message(f\"<b>‚ùå Error occurred:</b><pre>{str(e)}</pre>\")\n    \n        restart_button.layout.display = \"flex\"\n\n\n    threading.Thread(target=summarize_and_display).start()\n\n    text_input.value = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T15:31:26.692358Z","iopub.execute_input":"2025-04-20T15:31:26.692655Z","iopub.status.idle":"2025-04-20T15:31:26.712763Z","shell.execute_reply.started":"2025-04-20T15:31:26.692634Z","shell.execute_reply":"2025-04-20T15:31:26.711817Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üîÅ Step 12: Finalize UI Events, Restart Logic, and Launch EmotiBot Interface\n### üîß What This Step Does:\nThis final step connects all the buttons to their actions and brings the EmotiBot chatbot to life in the notebook:\n\n* üîö **End Chat button** triggers emotional summary and insights *(on_end_chat)*\n* üìÑ **Analyze Document button** invokes Gemini-powered document understanding *(handle_kaggle_file)*\n* üîÑ **Restart Session** clears state and starts a new chat *(restart_chat)*\n* üì§ **Send Button** and Enter key trigger live message handling *(handle_send)*\n* üñ•Ô∏è **Displays** the final chat UI using display *(chat_ui)*\n\n**It also:**\n   * Clears the previous session data.\n   * Resets all flags and UI elements.\n   * Adds Gemini‚Äôs initial friendly greeting from EmotiBot.\n\n### üß† So How Is GenAI Evaluation Used in this Step (lightweight):\n\n#### Step 12 enables GenAI Evaluation by connecting the ‚ÄúEnd Chat‚Äù button to the on_end_chat() function.\nThis function calls Gemini to:\n\n* Analyze emotional tone from the full conversation.\n* Generate a supportive summary.\n* Visualize emotions in a pie chart.\n* Suggest next-step reflections based on emotional insights","metadata":{}},{"cell_type":"code","source":"def insert_summary_message(html):\n    for i in reversed(range(len(chat_history))):\n        if chat_history[i] == \"__preparing_summary__\":\n            chat_history[i] = AIMessage(content=html)\n            render_chat()\n            return\n    chat_history.append(AIMessage(content=html))  # fallback\n    render_chat()\n\n\n    # Fallback if not found\n    chat_history.append(AIMessage(content=html))\n    render_chat()\n\n\ndef restart_chat(_=None):\n    global already_warned_user, session_ended, has_ended_message\n\n    chat_history.clear()\n    uploaded_docs.clear()\n    analyzed_files_set.clear()\n    currently_processing_file.clear()\n\n    already_warned_user = False\n    session_ended = False\n    has_ended_message = False  # Reset end message flag\n\n    chat_html.value = \"\"\n    input_bar.layout.display = \"flex\"\n    file_dropdown.layout.display = \"block\"\n    analyze_button.layout.display = \"block\"\n    restart_button.layout.display = \"none\"\n    end_chat_button.disabled = False\n\n    chat_history.append(AIMessage(content=\"üëã Hi again! I'm EmotiBot. How can I support you today?\"))\n    render_chat()\n\n# Global storage for doc analysis\nuploaded_doc_summary_html = \"\"\nuploaded_doc_emotions = {}\n\n# Button setup\nend_chat_button.on_click(on_end_chat)\n\nanalyze_button.on_click(handle_kaggle_file)\n\n\nrestart_button.on_click(restart_chat)\n\n\n# Reset chat\nchat_history.clear()\nchat_html.value = \"\"\n\n# Set up events\nsend_button.on_click(handle_send)\ntext_input.observe(on_key_press, names=\"value\")\ndisplay(chat_ui)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T15:31:33.514469Z","iopub.execute_input":"2025-04-20T15:31:33.514861Z","iopub.status.idle":"2025-04-20T15:31:33.526411Z","shell.execute_reply.started":"2025-04-20T15:31:33.514837Z","shell.execute_reply":"2025-04-20T15:31:33.525712Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## üïπÔ∏è How to Use EmotiBot\n### 1Ô∏è‚É£ Start Chat\nType your thoughts in the box and hit  **‚û§ Send**. EmotiBot replies with emotional support.\n\n### 2Ô∏è‚É£ Upload Document\nChoose a **.pdf or .txt** from the üìÑ **File Dropdown**, then click  **üîç Analyze Document** it analyze document and give the summary and üìà pie chart of emotions after the end of chat.\n\n### 3Ô∏è‚É£ End Session\nClick End Chat to get a üí¨ summary, üìà emotion pie chart, and üå± self-care suggestions.\n\n### 4Ô∏è‚É£ Restart Anytime\nClick **üîÑ Restart Session** to clear chat and begin again.\n\n‚úÖ Use it to reflect, analyze writing, or explore emotional tone in conversations & documents.","metadata":{}}]}